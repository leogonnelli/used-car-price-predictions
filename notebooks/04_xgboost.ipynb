{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === XGBoost Modeling Notebook ===\n",
    "# Testing new features: Target Encoding + Tax Features + Polynomial Features\n",
    "# Using XGBoost as a fast proxy (2-3 min) to test improvements before running LightGBM (16 min)\n",
    "\n",
    "# === 1. Imports ===\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(\"../\"))  # ensure src/ is importable\n",
    "\n",
    "# Reload modules to pick up latest changes\n",
    "import importlib\n",
    "import src.preprocess.preprocessing_pipeline\n",
    "import src.preprocess.encoding\n",
    "import src.preprocess.feature_engineering\n",
    "importlib.reload(src.preprocess.encoding)\n",
    "importlib.reload(src.preprocess.feature_engineering)\n",
    "importlib.reload(src.preprocess.preprocessing_pipeline)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.metrics import root_mean_squared_error, r2_score\n",
    "\n",
    "from src.load_data import load_train_data, load_test_data\n",
    "from src.preprocess.preprocessing_pipeline import PreprocessingPipeline\n",
    "\n",
    "print(\"✅ Modules reloaded - ready to use new features!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 2. Load data ===\n",
    "df = load_train_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 3. Prepare features and target (before preprocessing) ===\n",
    "X_raw = df.drop(columns=[\"price\"])\n",
    "y_raw = df[\"price\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 4. Train-test split (on raw data) ===\n",
    "X_train_raw, X_test_raw, y_train_raw, y_test_raw = train_test_split(\n",
    "    X_raw, y_raw, test_size=0.2, random_state=42\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 4a. Preprocess training data ===\n",
    "train_df = X_train_raw.copy()\n",
    "train_df[\"price\"] = y_train_raw\n",
    "\n",
    "pipeline = PreprocessingPipeline(\n",
    "    use_log_target=True,\n",
    "    drop_low_importance=True,\n",
    "    encode_data=True,\n",
    "    use_target_encoding=True  # Enable target encoding for better XGBoost performance\n",
    ")\n",
    "train_processed = pipeline.fit_transform(train_df)\n",
    "X_train = train_processed.drop(columns=[\"price\", \"log_price\"], errors='ignore')\n",
    "y_train = train_processed[\"log_price\"]\n",
    "\n",
    "print(f\"✅ Training data shape: {X_train.shape}\")\n",
    "print(f\"✅ Features with target encoding: {[col for col in X_train.columns if 'target_enc' in col]}\")\n",
    "print(f\"✅ New tax features: {[col for col in X_train.columns if 'tax' in col.lower()]}\")\n",
    "print(f\"✅ Polynomial features: {[col for col in X_train.columns if 'squared' in col or 'interaction' in col]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 4b. Preprocess test data (using fitted pipeline) ===\n",
    "test_df = X_test_raw.copy()\n",
    "test_df[\"price\"] = y_test_raw  # For consistency\n",
    "test_processed = pipeline.transform(test_df)\n",
    "X_test = test_processed.drop(columns=[\"price\", \"log_price\"], errors='ignore')\n",
    "y_test = test_processed[\"log_price\"] if \"log_price\" in test_processed.columns else y_test_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 5. Hyperparameter tuning with RandomizedSearchCV ===\n",
    "# Expanded parameter distribution for better exploration\n",
    "param_dist = {\n",
    "    \"n_estimators\": [300, 500, 800, 1000, 1200],\n",
    "    \"max_depth\": [5, 7, 9, 11],\n",
    "    \"learning_rate\": [0.01, 0.03, 0.05, 0.08, 0.1],\n",
    "    \"subsample\": [0.7, 0.8, 0.9, 1.0],\n",
    "    \"colsample_bytree\": [0.7, 0.8, 0.9, 1.0],\n",
    "    \"reg_alpha\": [0.0, 0.1, 0.3, 0.5],\n",
    "    \"reg_lambda\": [0.0, 0.1, 0.3, 0.5, 1.0],\n",
    "    \"min_child_weight\": [1, 3, 5, 7],\n",
    "    \"gamma\": [0.0, 0.1, 0.2]\n",
    "}\n",
    "\n",
    "base_model = XGBRegressor(random_state=42, n_jobs=-1)\n",
    "\n",
    "# RandomizedSearchCV: 50 iterations, 3-fold CV = ~150 fits (vs 729 with GridSearch)\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=base_model,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=50,\n",
    "    cv=3,\n",
    "    scoring=\"neg_root_mean_squared_error\",\n",
    "    verbose=2,\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(\"Starting hyperparameter search...\")\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "xgb_model = random_search.best_estimator_\n",
    "best_params = random_search.best_params_\n",
    "best_cv_rmse = -random_search.best_score_\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Best parameters:\", best_params)\n",
    "print(f\"Best CV RMSE (log scale): {best_cv_rmse:.5f}\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 6. Predict and evaluate ===\n",
    "y_pred = xgb_model.predict(X_test)\n",
    "rmse_log = root_mean_squared_error(y_test, y_pred)\n",
    "rmse_raw = root_mean_squared_error(np.expm1(y_test), np.expm1(y_pred))\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"XGBoost RMSE (log scale): {rmse_log:.5f}\")\n",
    "print(f\"XGBoost RMSE ($): {rmse_raw:.2f}\")\n",
    "print(f\"XGBoost R2 Score: {r2:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 7. Save model ===\n",
    "import joblib\n",
    "joblib.dump(xgb_model, \"../models/xgboost_model.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 8. Generate predictions for stacking ===\n",
    "# Note: This generates predictions on full training set, not OOF\n",
    "# For proper OOF predictions, use the oof_xgboost.py script\n",
    "full_train_processed = pipeline.fit_transform(df)\n",
    "X_full = full_train_processed.drop(columns=[\"price\", \"log_price\"], errors='ignore')\n",
    "train_preds = xgb_model.predict(X_full)\n",
    "pd.DataFrame({\n",
    "    \"xgb_oof_pred\": train_preds\n",
    "}).to_csv(\"../results/xgb_oof_train_preds.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 9. Load and preprocess test data ===\n",
    "test_df = load_test_data()\n",
    "# Use the same pipeline that was fitted on training data\n",
    "test_clean = pipeline.transform(test_df)\n",
    "\n",
    "# === 10. Predict and save submission ===\n",
    "test_preds = np.expm1(xgb_model.predict(test_clean))  # Convert log(price) back\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    \"ID\": test_df[\"ID\"],\n",
    "    \"Actual\": test_preds\n",
    "})\n",
    "submission.to_csv(\"../results/xgb_test_preds.csv\", index=False)\n",
    "print(\"✅ XGBoost submission saved to: ../results/xgb_test_preds.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

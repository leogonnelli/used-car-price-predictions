{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Performance Comparison & Visualizations\n",
    "\n",
    "This notebook creates visualizations for comparing model performances, suitable for PowerPoint presentation.\n",
    "\n",
    "**Models included:**\n",
    "- Linear Regression (baseline)\n",
    "- Random Forest\n",
    "- XGBoost\n",
    "- CatBoost\n",
    "- LightGBM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 1. Imports ===\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "\n",
    "# Set style for professional presentations\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Increase figure size and font for presentations\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 12\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['axes.titlesize'] = 16\n",
    "plt.rcParams['xtick.labelsize'] = 12\n",
    "plt.rcParams['ytick.labelsize'] = 12\n",
    "plt.rcParams['legend.fontsize'] = 12\n",
    "\n",
    "print(\"âœ… Imports complete\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 2. Load Model Performance Data ===\n",
    "# RMSE scores from validation/test sets (Kaggle test set)\n",
    "model_performance = {\n",
    "    'Linear Regression': 4103.64,  # Baseline from notebook 02\n",
    "    'Random Forest': 2907,  # Kaggle test set RMSE (worst model)\n",
    "    'XGBoost': 2476,  # From config\n",
    "    'CatBoost': 1982,  # From config\n",
    "    'LightGBM': 1892,  # From config (best individual model)\n",
    "}\n",
    "\n",
    "# Create DataFrame for easier manipulation\n",
    "df_performance = pd.DataFrame({\n",
    "    'Model': list(model_performance.keys()),\n",
    "    'RMSE': list(model_performance.values())\n",
    "})\n",
    "\n",
    "# Sort by RMSE (best to worst)\n",
    "df_performance = df_performance.sort_values('RMSE')\n",
    "\n",
    "print(\"ðŸ“Š Model Performance Data:\")\n",
    "print(df_performance.to_string(index=False))\n",
    "print(f\"\\nâœ… Best Model: {df_performance.iloc[0]['Model']} with RMSE = ${df_performance.iloc[0]['RMSE']:.2f}\")\n",
    "print(f\"ðŸ“‰ Improvement over baseline: {((model_performance['Linear Regression'] - df_performance.iloc[0]['RMSE']) / model_performance['Linear Regression'] * 100):.1f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization 1: RMSE Comparison Bar Chart\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 3. RMSE Comparison Bar Chart ===\n",
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "\n",
    "# Create color gradient (green = best, red = worst)\n",
    "colors = ['#2ecc71', '#3498db', '#9b59b6', '#e67e22', '#e74c3c']\n",
    "bars = ax.barh(df_performance['Model'], df_performance['RMSE'], color=colors)\n",
    "\n",
    "# Add value labels on bars\n",
    "for i, (model, rmse) in enumerate(zip(df_performance['Model'], df_performance['RMSE'])):\n",
    "    ax.text(rmse + 50, i, f'${rmse:.0f}', \n",
    "            va='center', fontsize=12, fontweight='bold')\n",
    "\n",
    "# Customize chart\n",
    "ax.set_xlabel('RMSE (Root Mean Squared Error)', fontsize=14, fontweight='bold')\n",
    "ax.set_ylabel('Model', fontsize=14, fontweight='bold')\n",
    "ax.set_title('Model Performance Comparison: RMSE Scores', \n",
    "             fontsize=18, fontweight='bold', pad=20)\n",
    "ax.grid(axis='x', alpha=0.3, linestyle='--')\n",
    "\n",
    "# Highlight best model\n",
    "best_idx = df_performance['RMSE'].idxmin()\n",
    "bars[df_performance.index.get_loc(best_idx)].set_edgecolor('gold')\n",
    "bars[df_performance.index.get_loc(best_idx)].set_linewidth(3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/model_rmse_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ… Saved: model_rmse_comparison.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization 2: Performance Improvement Over Baseline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 4. Improvement Over Baseline ===\n",
    "baseline_rmse = model_performance['Linear Regression']\n",
    "df_performance['Improvement_%'] = ((baseline_rmse - df_performance['RMSE']) / baseline_rmse) * 100\n",
    "df_performance['Improvement_$'] = baseline_rmse - df_performance['RMSE']\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "\n",
    "# Create horizontal bar chart\n",
    "bars = ax.barh(df_performance['Model'], df_performance['Improvement_%'], \n",
    "               color=colors)\n",
    "\n",
    "# Add value labels\n",
    "for i, (model, pct, dollar) in enumerate(zip(df_performance['Model'], \n",
    "                                               df_performance['Improvement_%'],\n",
    "                                               df_performance['Improvement_$'])):\n",
    "    ax.text(pct + 0.5, i, f'{pct:.1f}% (${dollar:.0f})', \n",
    "            va='center', fontsize=12, fontweight='bold')\n",
    "\n",
    "ax.set_xlabel('Improvement Over Baseline (%)', fontsize=14, fontweight='bold')\n",
    "ax.set_ylabel('Model', fontsize=14, fontweight='bold')\n",
    "ax.set_title('Model Performance Improvement Over Linear Regression Baseline', \n",
    "             fontsize=18, fontweight='bold', pad=20)\n",
    "ax.grid(axis='x', alpha=0.3, linestyle='--')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/model_improvement.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ… Saved: model_improvement.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization 3: Prediction Distribution Comparison\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 5. Load Test Predictions for Distribution Analysis ===\n",
    "results_dir = Path('../results')\n",
    "\n",
    "# Load predictions from different models\n",
    "predictions = {}\n",
    "\n",
    "# LightGBM\n",
    "lgbm_preds = pd.read_csv(results_dir / 'lightgbm_test_preds.csv')\n",
    "predictions['LightGBM'] = lgbm_preds['Actual'].values\n",
    "\n",
    "# CatBoost\n",
    "catboost_preds = pd.read_csv(results_dir / 'catboost_test_preds.csv')\n",
    "predictions['CatBoost'] = catboost_preds['Actual'].values\n",
    "\n",
    "# XGBoost\n",
    "xgb_preds = pd.read_csv(results_dir / 'xgb_test_preds.csv')\n",
    "predictions['XGBoost'] = xgb_preds['Actual'].values\n",
    "\n",
    "# Random Forest\n",
    "rf_preds = pd.read_csv(results_dir / 'tuned_rf_submission.csv')\n",
    "predictions['Random Forest'] = rf_preds['Actual'].values\n",
    "\n",
    "print(f\"âœ… Loaded predictions for {len(predictions)} models\")\n",
    "print(f\"   Sample size: {len(predictions['LightGBM'])} predictions per model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 6. Prediction Distribution Comparison ===\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Plot 1: Histogram overlay\n",
    "ax1 = axes[0]\n",
    "for model_name, preds in predictions.items():\n",
    "    ax1.hist(preds, bins=50, alpha=0.6, label=model_name, density=True)\n",
    "ax1.set_xlabel('Predicted Price ($)', fontsize=12, fontweight='bold')\n",
    "ax1.set_ylabel('Density', fontsize=12, fontweight='bold')\n",
    "ax1.set_title('Prediction Distribution Comparison', fontsize=14, fontweight='bold')\n",
    "ax1.legend(fontsize=10)\n",
    "ax1.grid(alpha=0.3)\n",
    "\n",
    "# Plot 2: Box plot\n",
    "ax2 = axes[1]\n",
    "df_preds = pd.DataFrame(predictions)\n",
    "box = ax2.boxplot([df_preds[col] for col in df_preds.columns], \n",
    "                  labels=df_preds.columns, patch_artist=True)\n",
    "for patch, color in zip(box['boxes'], colors[:len(box['boxes'])]):\n",
    "    patch.set_facecolor(color)\n",
    "    patch.set_alpha(0.7)\n",
    "ax2.set_ylabel('Predicted Price ($)', fontsize=12, fontweight='bold')\n",
    "ax2.set_title('Prediction Distribution: Box Plot', fontsize=14, fontweight='bold')\n",
    "ax2.tick_params(axis='x', rotation=45)\n",
    "ax2.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Plot 3: Violin plot\n",
    "ax3 = axes[2]\n",
    "parts = ax3.violinplot([df_preds[col] for col in df_preds.columns], \n",
    "                       positions=range(len(df_preds.columns)),\n",
    "                       showmeans=True, showmedians=True)\n",
    "ax3.set_xticks(range(len(df_preds.columns)))\n",
    "ax3.set_xticklabels(df_preds.columns, rotation=45, ha='right')\n",
    "ax3.set_ylabel('Predicted Price ($)', fontsize=12, fontweight='bold')\n",
    "ax3.set_title('Prediction Distribution: Violin Plot', fontsize=14, fontweight='bold')\n",
    "ax3.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Plot 4: Summary statistics\n",
    "ax4 = axes[3]\n",
    "ax4.axis('off')\n",
    "stats_data = []\n",
    "for model_name, preds in predictions.items():\n",
    "    stats_data.append({\n",
    "        'Model': model_name,\n",
    "        'Mean': f\"${preds.mean():,.0f}\",\n",
    "        'Median': f\"${np.median(preds):,.0f}\",\n",
    "        'Std': f\"${preds.std():,.0f}\",\n",
    "        'Min': f\"${preds.min():,.0f}\",\n",
    "        'Max': f\"${preds.max():,.0f}\"\n",
    "    })\n",
    "stats_df = pd.DataFrame(stats_data)\n",
    "table = ax4.table(cellText=stats_df.values, colLabels=stats_df.columns,\n",
    "                  cellLoc='center', loc='center', bbox=[0, 0, 1, 1])\n",
    "table.auto_set_font_size(False)\n",
    "table.set_fontsize(10)\n",
    "table.scale(1, 2)\n",
    "ax4.set_title('Prediction Summary Statistics', fontsize=14, fontweight='bold', pad=20)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/prediction_distributions.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ… Saved: prediction_distributions.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization 4: Model Correlation Heatmap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 7. Model Prediction Correlation ===\n",
    "correlation_matrix = df_preds.corr()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, fmt='.3f', cmap='coolwarm', \n",
    "            center=0.95, vmin=0.9, vmax=1.0, square=True, \n",
    "            linewidths=2, cbar_kws={'label': 'Correlation Coefficient'},\n",
    "            ax=ax)\n",
    "ax.set_title('Model Prediction Correlation Matrix', \n",
    "             fontsize=16, fontweight='bold', pad=20)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/model_correlation.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ… Saved: model_correlation.png\")\n",
    "print(\"\\nðŸ“Š Correlation Insights:\")\n",
    "print(f\"   High correlation ({correlation_matrix.min().min():.3f} - {correlation_matrix.max().max():.3f})\")\n",
    "print(\"   indicates models make similar predictions, limiting ensemble benefits.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization 5: RMSE vs Model Complexity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 8. Model Complexity vs Performance ===\n",
    "# Define complexity scores (subjective, based on model type)\n",
    "complexity_scores = {\n",
    "    'Linear Regression': 1,  # Simplest\n",
    "    'Random Forest': 3,\n",
    "    'XGBoost': 4,\n",
    "    'CatBoost': 4,\n",
    "    'LightGBM': 4,\n",
    "}\n",
    "\n",
    "df_complexity = df_performance.copy()\n",
    "df_complexity['Complexity'] = df_complexity['Model'].map(complexity_scores)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "# Scatter plot\n",
    "scatter = ax.scatter(df_complexity['Complexity'], df_complexity['RMSE'], \n",
    "                    s=300, c=colors, alpha=0.7, edgecolors='black', linewidth=2)\n",
    "\n",
    "# Add labels\n",
    "for idx, row in df_complexity.iterrows():\n",
    "    ax.annotate(row['Model'], \n",
    "               (row['Complexity'], row['RMSE']),\n",
    "               xytext=(5, 5), textcoords='offset points',\n",
    "               fontsize=11, fontweight='bold')\n",
    "\n",
    "ax.set_xlabel('Model Complexity (1=Simple, 4=Complex)', fontsize=14, fontweight='bold')\n",
    "ax.set_ylabel('RMSE ($)', fontsize=14, fontweight='bold')\n",
    "ax.set_title('Model Performance vs Complexity Trade-off', \n",
    "             fontsize=16, fontweight='bold', pad=20)\n",
    "ax.grid(alpha=0.3, linestyle='--')\n",
    "ax.set_xticks(range(1, 5))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/complexity_vs_performance.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ… Saved: complexity_vs_performance.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization 6: Performance Ranking\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 9. Performance Ranking Chart ===\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "# Create ranking (1 = best)\n",
    "df_performance['Rank'] = range(1, len(df_performance) + 1)\n",
    "\n",
    "# Horizontal bar chart\n",
    "bars = ax.barh(df_performance['Model'], df_performance['RMSE'], \n",
    "               color=colors, alpha=0.8)\n",
    "\n",
    "# Add rank labels\n",
    "for i, (model, rmse, rank) in enumerate(zip(df_performance['Model'], \n",
    "                                             df_performance['RMSE'],\n",
    "                                             df_performance['Rank'])):\n",
    "    # Rank badge\n",
    "    ax.text(rmse - 200, i, f'#{rank}', \n",
    "            va='center', ha='right', fontsize=16, fontweight='bold',\n",
    "            bbox=dict(boxstyle='round', facecolor='white', edgecolor='black', pad=0.5))\n",
    "    # RMSE value\n",
    "    ax.text(rmse + 50, i, f'${rmse:.0f}', \n",
    "            va='center', fontsize=12, fontweight='bold')\n",
    "\n",
    "ax.set_xlabel('RMSE ($)', fontsize=14, fontweight='bold')\n",
    "ax.set_ylabel('Model', fontsize=14, fontweight='bold')\n",
    "ax.set_title('Model Performance Ranking', fontsize=18, fontweight='bold', pad=20)\n",
    "ax.grid(axis='x', alpha=0.3, linestyle='--')\n",
    "\n",
    "# Invert y-axis so best is at top\n",
    "ax.invert_yaxis()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/model_ranking.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ… Saved: model_ranking.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary Statistics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 10. Final Summary Table ===\n",
    "print(\"=\"*70)\n",
    "print(\"MODEL PERFORMANCE SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\n{'Model':<25} {'RMSE ($)':<15} {'Rank':<10} {'Improvement %':<15}\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "for idx, row in df_performance.iterrows():\n",
    "    improvement = ((baseline_rmse - row['RMSE']) / baseline_rmse) * 100\n",
    "    print(f\"{row['Model']:<25} ${row['RMSE']:<14.0f} {row['Rank']:<10} {improvement:<14.1f}%\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(f\"ðŸ† Best Model: {df_performance.iloc[0]['Model']}\")\n",
    "print(f\"   RMSE: ${df_performance.iloc[0]['RMSE']:.2f}\")\n",
    "print(f\"   Improvement over baseline: {((baseline_rmse - df_performance.iloc[0]['RMSE']) / baseline_rmse * 100):.1f}%\")\n",
    "print(f\"   Absolute improvement: ${baseline_rmse - df_performance.iloc[0]['RMSE']:.2f}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Save summary to CSV\n",
    "df_performance.to_csv('../results/model_performance_summary.csv', index=False)\n",
    "print(\"\\nâœ… Summary saved to: model_performance_summary.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generated Files\n",
    "\n",
    "All visualizations have been saved to the `results/` folder:\n",
    "\n",
    "1. **model_rmse_comparison.png** - Bar chart comparing RMSE scores\n",
    "2. **model_improvement.png** - Improvement over baseline\n",
    "3. **prediction_distributions.png** - Distribution comparisons\n",
    "4. **model_correlation.png** - Correlation heatmap\n",
    "5. **complexity_vs_performance.png** - Complexity trade-off\n",
    "6. **model_ranking.png** - Performance ranking\n",
    "7. **model_performance_summary.csv** - Summary statistics\n",
    "\n",
    "These files are ready for use in your PowerPoint presentation!\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

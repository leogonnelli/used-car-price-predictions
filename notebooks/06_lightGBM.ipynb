{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# LightGBM Model (log-target)\n",
    "\n",
    "Train a tuned LightGBM regressor using the leakage-free preprocessing pipeline, then export predictions for Kaggle.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Imports ===\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.abspath(\"../\"))\n",
    "\n",
    "# Reload modules to pick up latest changes\n",
    "import importlib\n",
    "import src.preprocess.preprocessing_pipeline\n",
    "import src.preprocess.encoding\n",
    "import src.preprocess.feature_engineering\n",
    "importlib.reload(src.preprocess.encoding)\n",
    "importlib.reload(src.preprocess.feature_engineering)\n",
    "importlib.reload(src.preprocess.preprocessing_pipeline)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "import joblib\n",
    "\n",
    "from src.load_data import load_train_data, load_test_data\n",
    "from src.preprocess.preprocessing_pipeline import PreprocessingPipeline\n",
    "\n",
    "print(\"✅ Modules reloaded - ready to use new features!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Load data & create holdout split ===\n",
    "df = load_train_data()\n",
    "\n",
    "X_raw = df.drop(columns=[\"price\"])\n",
    "y_raw = df[\"price\"]\n",
    "\n",
    "X_train_raw, X_val_raw, y_train_raw, y_val_raw = train_test_split(\n",
    "    X_raw,\n",
    "    y_raw,\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Train rows: {X_train_raw.shape[0]} | Val rows: {X_val_raw.shape[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Preprocess training data ===\n",
    "train_df = X_train_raw.copy()\n",
    "train_df[\"price\"] = y_train_raw\n",
    "\n",
    "pipeline = PreprocessingPipeline(\n",
    "    use_log_target=True,\n",
    "    drop_low_importance=True,\n",
    "    encode_data=False,  # keep categoricals for native LightGBM handling\n",
    "    use_target_encoding=True  # Add target encoding as additional features\n",
    ")\n",
    "train_processed = pipeline.fit_transform(train_df)\n",
    "\n",
    "X_train = train_processed.drop(columns=[\"price\", \"log_price\"], errors=\"ignore\").copy()\n",
    "y_train = train_processed[\"log_price\"].copy()\n",
    "\n",
    "# Convert all object/string columns to category for LightGBM\n",
    "# LightGBM requires: numeric (int/float/bool) or category dtype\n",
    "categorical_candidates = [\"model\", \"brand\", \"transmission\", \"fuelType\", \"brand_model\"]\n",
    "cat_features = []\n",
    "for col in X_train.columns:\n",
    "    if X_train[col].dtype == 'object' or col in categorical_candidates:\n",
    "        X_train[col] = X_train[col].astype(\"category\")\n",
    "        if col in categorical_candidates:\n",
    "            cat_features.append(col)\n",
    "\n",
    "# Ensure all numeric columns are proper types\n",
    "for col in X_train.columns:\n",
    "    if X_train[col].dtype.name == 'category':\n",
    "        continue\n",
    "    # Convert any remaining object columns to numeric or drop them\n",
    "    if X_train[col].dtype == 'object':\n",
    "        # Try to convert to numeric, if fails, drop it\n",
    "        try:\n",
    "            X_train[col] = pd.to_numeric(X_train[col], errors='coerce')\n",
    "        except:\n",
    "            X_train = X_train.drop(columns=[col])\n",
    "\n",
    "print(f\"✅ X_train shape: {X_train.shape}\")\n",
    "print(f\"✅ Categorical features (for native handling): {cat_features}\")\n",
    "print(f\"✅ Target-encoded features: {[col for col in X_train.columns if 'target_enc' in col]}\")\n",
    "print(f\"✅ Tax features: {[col for col in X_train.columns if 'tax' in col.lower()]}\")\n",
    "print(f\"✅ Polynomial features: {[col for col in X_train.columns if 'squared' in col or 'interaction' in col]}\")\n",
    "print(f\"✅ Column dtypes: {X_train.dtypes.value_counts().to_dict()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Preprocess validation data ===\n",
    "val_df = X_val_raw.copy()\n",
    "val_df[\"price\"] = y_val_raw\n",
    "val_processed = pipeline.transform(val_df)\n",
    "\n",
    "X_val = val_processed.drop(columns=[\"price\", \"log_price\"], errors=\"ignore\").copy()\n",
    "if \"log_price\" in val_processed.columns:\n",
    "    y_val = val_processed[\"log_price\"].copy()\n",
    "else:\n",
    "    y_val = np.log1p(y_val_raw)\n",
    "\n",
    "# Convert all object/string columns to category for LightGBM (same as training)\n",
    "for col in X_val.columns:\n",
    "    if X_val[col].dtype == 'object' or col in categorical_candidates:\n",
    "        if col in X_train.columns and X_train[col].dtype.name == 'category':\n",
    "            # Use same categories as training\n",
    "            X_val[col] = pd.Categorical(X_val[col], categories=X_train[col].cat.categories)\n",
    "        else:\n",
    "            X_val[col] = X_val[col].astype(\"category\")\n",
    "\n",
    "# Ensure all numeric columns are proper types (same as training)\n",
    "for col in X_val.columns:\n",
    "    if X_val[col].dtype.name == 'category':\n",
    "        continue\n",
    "    if X_val[col].dtype == 'object':\n",
    "        try:\n",
    "            X_val[col] = pd.to_numeric(X_val[col], errors='coerce')\n",
    "        except:\n",
    "            if col in X_val.columns:\n",
    "                X_val = X_val.drop(columns=[col])\n",
    "\n",
    "# Ensure X_val has same columns as X_train\n",
    "X_val = X_val[X_train.columns]\n",
    "\n",
    "print(f\"✅ X_val shape: {X_val.shape}\")\n",
    "print(f\"✅ X_val column dtypes: {X_val.dtypes.value_counts().to_dict()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Hyperparameter search (RandomizedSearchCV) ===\n",
    "param_dist = {\n",
    "    \"num_leaves\": [31, 63, 95, 127],\n",
    "    \"max_depth\": [-1, 8, 10, 12],\n",
    "    \"learning_rate\": [0.01, 0.02, 0.03, 0.05],\n",
    "    \"n_estimators\": [600, 900, 1200, 1500],\n",
    "    \"subsample\": [0.7, 0.85, 1.0],\n",
    "    \"colsample_bytree\": [0.7, 0.85, 1.0],\n",
    "    \"reg_alpha\": [0.0, 0.1, 0.3, 0.5],\n",
    "    \"reg_lambda\": [0.0, 0.1, 0.3, 0.5],\n",
    "    \"min_child_weight\": [1, 5, 10],\n",
    "    \"min_child_samples\": [20, 40, 60]\n",
    "}\n",
    "\n",
    "base_lgbm = LGBMRegressor(\n",
    "    objective=\"regression\",\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    boosting_type=\"gbdt\"\n",
    ")\n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=base_lgbm,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=40,\n",
    "    scoring=\"neg_root_mean_squared_error\",\n",
    "    cv=5,\n",
    "    verbose=2,\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "fit_params = {\"categorical_feature\": cat_features}\n",
    "random_search.fit(X_train, y_train, **fit_params)\n",
    "\n",
    "best_model = random_search.best_estimator_\n",
    "best_params = random_search.best_params_\n",
    "best_cv_rmse = -random_search.best_score_\n",
    "\n",
    "print(\"Best params:\", best_params)\n",
    "print(f\"Best CV RMSE (log): {best_cv_rmse:.5f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Validation performance ===\n",
    "val_preds = best_model.predict(X_val)\n",
    "\n",
    "val_rmse_log = root_mean_squared_error(y_val, val_preds)\n",
    "val_rmse_raw = root_mean_squared_error(np.expm1(y_val), np.expm1(val_preds))\n",
    "print(f\"Validation RMSE (log): {val_rmse_log:.5f}\")\n",
    "print(f\"Validation RMSE ($): {val_rmse_raw:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Train final model on full dataset ===\n",
    "full_df = load_train_data()\n",
    "final_pipeline = PreprocessingPipeline(\n",
    "    use_log_target=True,\n",
    "    drop_low_importance=True,\n",
    "    encode_data=False\n",
    ")\n",
    "full_processed = final_pipeline.fit_transform(full_df)\n",
    "\n",
    "X_full = full_processed.drop(columns=[\"price\", \"log_price\"], errors=\"ignore\").copy()\n",
    "y_full = full_processed[\"log_price\"].copy()\n",
    "\n",
    "cat_features_full = [col for col in categorical_candidates if col in X_full.columns]\n",
    "for col in cat_features_full:\n",
    "    X_full[col] = X_full[col].astype(\"category\")\n",
    "\n",
    "final_model = LGBMRegressor(\n",
    "    **best_params,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    boosting_type=\"gbdt\"\n",
    ")\n",
    "final_model.fit(X_full, y_full, categorical_feature=cat_features_full)\n",
    "\n",
    "model_path = \"../models/lightgbm_model.joblib\"\n",
    "joblib.dump({\n",
    "    \"model\": final_model,\n",
    "    \"pipeline\": final_pipeline,\n",
    "    \"categorical_features\": cat_features_full,\n",
    "    \"feature_columns\": X_full.columns.tolist()\n",
    "}, model_path)\n",
    "print(f\"Saved LightGBM model + pipeline to {model_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Generate Kaggle submission ===\n",
    "test_df = load_test_data()\n",
    "test_processed = final_pipeline.transform(test_df)\n",
    "\n",
    "X_test = test_processed.copy()\n",
    "missing_cols = [col for col in X_full.columns if col not in X_test.columns]\n",
    "for col in missing_cols:\n",
    "    X_test[col] = 0\n",
    "X_test = X_test[X_full.columns]\n",
    "\n",
    "for col in cat_features_full:\n",
    "    if col in X_test.columns:\n",
    "        X_test[col] = pd.Categorical(X_test[col], categories=X_full[col].cat.categories)\n",
    "\n",
    "test_preds_log = final_model.predict(X_test)\n",
    "test_preds = np.expm1(test_preds_log)\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    \"ID\": test_df[\"ID\"],\n",
    "    \"Actual\": test_preds\n",
    "})\n",
    "submission_path = \"../results/lightgbm_test_preds.csv\"\n",
    "submission.to_csv(submission_path, index=False)\n",
    "print(f\"Saved LightGBM submission to {submission_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

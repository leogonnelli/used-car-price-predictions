{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Blending Experiments\n",
    "\n",
    "This notebook experiments with different blending strategies to improve predictions beyond individual models.\n",
    "\n",
    "**Current Best Models:**\n",
    "- LightGBM: 1892 RMSE (3rd place! ðŸŽ‰)\n",
    "- CatBoost: ~2023 RMSE (update after running with new features)\n",
    "- XGBoost: ~2476 RMSE (update after running with new features)\n",
    "\n",
    "**Goal:** Blend models to achieve < 1850 RMSE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Load Predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(\"../\"))\n",
    "\n",
    "# Reload modules to pick up latest changes\n",
    "import importlib\n",
    "import ensemble.blending\n",
    "import ensemble.config\n",
    "importlib.reload(ensemble.blending)\n",
    "importlib.reload(ensemble.config)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from ensemble.blending import WeightedBlender\n",
    "from ensemble.config import MODEL_CONFIGS, BEST_MODELS\n",
    "\n",
    "print(\"âœ… Setup complete! Modules reloaded.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load predictions for best models only\n",
    "best_configs = {name: MODEL_CONFIGS[name] for name in BEST_MODELS}\n",
    "\n",
    "blender = WeightedBlender(best_configs)\n",
    "blender.load_predictions(results_dir=\"../results\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Experiment with Different Blending Methods\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 1: Inverse RMSE weighting (recommended)\n",
    "blender.compute_weights(method='inverse_rmse')\n",
    "blended_inv_rmse = blender.blend()\n",
    "\n",
    "print(\"\\nðŸ“Š Inverse RMSE Weighting:\")\n",
    "print(f\"   Predictions range: {blended_inv_rmse.min():.2f} - {blended_inv_rmse.max():.2f}\")\n",
    "print(f\"   Mean prediction: {blended_inv_rmse.mean():.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 2: Equal weighting\n",
    "blender.compute_weights(method='equal')\n",
    "blended_equal = blender.blend()\n",
    "\n",
    "print(\"\\nðŸ“Š Equal Weighting:\")\n",
    "print(f\"   Predictions range: {blended_equal.min():.2f} - {blended_equal.max():.2f}\")\n",
    "print(f\"   Mean prediction: {blended_equal.mean():.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 3: Manual weighting (experiment with different ratios)\n",
    "# Example: Give more weight to LightGBM since it's the best\n",
    "manual_configs = {\n",
    "    'lightgbm': {'path': MODEL_CONFIGS['lightgbm']['path'], 'rmse': 1892, 'weight': 0.7},\n",
    "    'catboost': {'path': MODEL_CONFIGS['catboost']['path'], 'rmse': 2023, 'weight': 0.3}\n",
    "}\n",
    "\n",
    "blender_manual = WeightedBlender(manual_configs)\n",
    "blender_manual.load_predictions(results_dir=\"../results\")\n",
    "blender_manual.compute_weights(method='manual')\n",
    "blended_manual = blender_manual.blend()\n",
    "\n",
    "print(\"\\nðŸ“Š Manual Weighting (70% LightGBM, 30% CatBoost):\")\n",
    "print(f\"   Predictions range: {blended_manual.min():.2f} - {blended_manual.max():.2f}\")\n",
    "print(f\"   Mean prediction: {blended_manual.mean():.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2b. More Aggressive Weighting (Favor LightGBM)\n",
    "Since models are highly correlated (0.997), we need to give much more weight to the best model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 4: Inverse RMSE squared (more aggressive, favors best model)\n",
    "blender.compute_weights(method='inverse_rmse_squared')\n",
    "blended_inv_rmse_sq = blender.blend()\n",
    "\n",
    "print(\"\\nðŸ“Š Inverse RMSE Squared Weighting (More Aggressive):\")\n",
    "print(f\"   Predictions range: {blended_inv_rmse_sq.min():.2f} - {blended_inv_rmse_sq.max():.2f}\")\n",
    "print(f\"   Mean prediction: {blended_inv_rmse_sq.mean():.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 5: Test different LightGBM weights (80%, 85%, 90%, 95%)\n",
    "print(\"Testing different LightGBM weights:\\n\")\n",
    "results = {}\n",
    "\n",
    "for lgbm_weight in [0.80, 0.85, 0.90, 0.95]:\n",
    "    manual_configs = {\n",
    "        'lightgbm': {'path': MODEL_CONFIGS['lightgbm']['path'], 'rmse': 1892, 'weight': lgbm_weight},\n",
    "        'catboost': {'path': MODEL_CONFIGS['catboost']['path'], 'rmse': 1982, 'weight': 1.0 - lgbm_weight}\n",
    "    }\n",
    "    \n",
    "    blender_test = WeightedBlender(manual_configs)\n",
    "    blender_test.load_predictions(results_dir=\"../results\")\n",
    "    blender_test.compute_weights(method='manual')\n",
    "    blended_test = blender_test.blend()\n",
    "    \n",
    "    results[lgbm_weight] = blended_test\n",
    "    print(f\"  {lgbm_weight*100:.0f}% LightGBM: mean={blended_test.mean():.2f}, \"\n",
    "          f\"range=[{blended_test.min():.2f}, {blended_test.max():.2f}]\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 6: Best only (just LightGBM) - for comparison\n",
    "blender.compute_weights(method='best_only')\n",
    "blended_best_only = blender.blend()\n",
    "\n",
    "print(\"\\nðŸ“Š Best Only (LightGBM):\")\n",
    "print(f\"   Predictions range: {blended_best_only.min():.2f} - {blended_best_only.max():.2f}\")\n",
    "print(f\"   Mean prediction: {blended_best_only.mean():.2f}\")\n",
    "print(\"\\n   Note: This should match LightGBM predictions exactly\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Compare Predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare all blending methods\n",
    "comparison = pd.DataFrame({\n",
    "    'lightgbm': blender.predictions['lightgbm'],\n",
    "    'catboost': blender.predictions['catboost'],\n",
    "    'blended_inv_rmse': blended_inv_rmse,\n",
    "    'blended_inv_rmse_sq': blended_inv_rmse_sq,\n",
    "    'blended_manual_70': blended_manual,\n",
    "    'blended_90_lgbm': results[0.90],\n",
    "    'blended_95_lgbm': results[0.95],\n",
    "    'blended_best_only': blended_best_only\n",
    "})\n",
    "\n",
    "print(\"ðŸ“Š Prediction Statistics (selected methods):\")\n",
    "print(comparison[['lightgbm', 'catboost', 'blended_inv_rmse', 'blended_inv_rmse_sq', \n",
    "                  'blended_90_lgbm', 'blended_95_lgbm']].describe())\n",
    "\n",
    "print(\"\\nðŸ“Š Correlation with LightGBM (higher = more similar):\")\n",
    "corr_with_lgbm = comparison.corr()['lightgbm'].sort_values(ascending=False)\n",
    "print(corr_with_lgbm.round(4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Save Best Blended Predictions\n",
    "\n",
    "**Recommendation:** Since models are highly correlated, try:\n",
    "1. **90-95% LightGBM** blend (may slightly improve)\n",
    "2. **Best only (LightGBM)** if blending doesn't help\n",
    "\n",
    "The 90% LightGBM blend might capture small complementary signals from CatBoost.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save 90% LightGBM blend (recommended - may capture small complementary signals)\n",
    "manual_configs_90 = {\n",
    "    'lightgbm': {'path': MODEL_CONFIGS['lightgbm']['path'], 'rmse': 1892, 'weight': 0.90},\n",
    "    'catboost': {'path': MODEL_CONFIGS['catboost']['path'], 'rmse': 1982, 'weight': 0.10}\n",
    "}\n",
    "\n",
    "blender_90 = WeightedBlender(manual_configs_90)\n",
    "blender_90.load_predictions(results_dir=\"../results\")\n",
    "blender_90.compute_weights(method='manual')\n",
    "submission_90 = blender_90.save_blended(\n",
    "    output_path=\"../results/ensemble_blended_90_lgbm.csv\"\n",
    ")\n",
    "\n",
    "print(f\"\\nâœ… 90% LightGBM blend saved!\")\n",
    "print(f\"   First few predictions:\")\n",
    "print(submission_90.head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4b. Fine-tune Around 90% (Optimal Range)\n",
    "\n",
    "Since 90% performed best, let's test values around it to find the exact sweet spot.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tune around 90%: test 87%, 88%, 89%, 90%, 91%, 92%\n",
    "print(\"Fine-tuning blend ratio around 90%:\\n\")\n",
    "fine_tune_results = {}\n",
    "\n",
    "for lgbm_weight in [0.87, 0.88, 0.89, 0.90, 0.91, 0.92]:\n",
    "    manual_configs = {\n",
    "        'lightgbm': {'path': MODEL_CONFIGS['lightgbm']['path'], 'rmse': 1892, 'weight': lgbm_weight},\n",
    "        'catboost': {'path': MODEL_CONFIGS['catboost']['path'], 'rmse': 1982, 'weight': 1.0 - lgbm_weight}\n",
    "    }\n",
    "    \n",
    "    blender_fine = WeightedBlender(manual_configs)\n",
    "    blender_fine.load_predictions(results_dir=\"../results\")\n",
    "    blender_fine.compute_weights(method='manual')\n",
    "    blended_fine = blender_fine.blend()\n",
    "    \n",
    "    fine_tune_results[lgbm_weight] = blended_fine\n",
    "    print(f\"  {lgbm_weight*100:.0f}% LightGBM: mean={blended_fine.mean():.2f}, \"\n",
    "          f\"std={blended_fine.std():.2f}, range=[{blended_fine.min():.2f}, {blended_fine.max():.2f}]\")\n",
    "\n",
    "print(\"\\nðŸ’¡ Submit these to Kaggle to find the exact optimal ratio!\")\n",
    "print(\"   Current best: 90% LightGBM = 1889.03 RMSE\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save a few promising candidates for submission\n",
    "for lgbm_weight in [0.88, 0.89, 0.91, 0.92]:\n",
    "    manual_configs = {\n",
    "        'lightgbm': {'path': MODEL_CONFIGS['lightgbm']['path'], 'rmse': 1892, 'weight': lgbm_weight},\n",
    "        'catboost': {'path': MODEL_CONFIGS['catboost']['path'], 'rmse': 1982, 'weight': 1.0 - lgbm_weight}\n",
    "    }\n",
    "    \n",
    "    blender_candidate = WeightedBlender(manual_configs)\n",
    "    blender_candidate.load_predictions(results_dir=\"../results\")\n",
    "    blender_candidate.compute_weights(method='manual')\n",
    "    blender_candidate.save_blended(\n",
    "        output_path=f\"../results/ensemble_blended_{int(lgbm_weight*100)}_lgbm.csv\"\n",
    "    )\n",
    "\n",
    "print(\"\\nâœ… Fine-tuned blends saved! Test on Kaggle to find the optimal ratio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optionally save 95% LightGBM blend\n",
    "manual_configs_95 = {\n",
    "    'lightgbm': {'path': MODEL_CONFIGS['lightgbm']['path'], 'rmse': 1892, 'weight': 0.95},\n",
    "    'catboost': {'path': MODEL_CONFIGS['catboost']['path'], 'rmse': 1982, 'weight': 0.05}\n",
    "}\n",
    "\n",
    "blender_95 = WeightedBlender(manual_configs_95)\n",
    "blender_95.load_predictions(results_dir=\"../results\")\n",
    "blender_95.compute_weights(method='manual')\n",
    "submission_95 = blender_95.save_blended(\n",
    "    output_path=\"../results/ensemble_blended_95_lgbm.csv\"\n",
    ")\n",
    "\n",
    "print(f\"\\nâœ… 95% LightGBM blend saved!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Compare Predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare individual vs blended predictions\n",
    "comparison = pd.DataFrame({\n",
    "    'lightgbm': blender.predictions['lightgbm'],\n",
    "    'catboost': blender.predictions['catboost'],\n",
    "    'blended_inv_rmse': blended_inv_rmse,\n",
    "    'blended_equal': blended_equal,\n",
    "    'blended_manual': blended_manual\n",
    "})\n",
    "\n",
    "print(\"ðŸ“Š Prediction Statistics:\")\n",
    "print(comparison.describe())\n",
    "\n",
    "print(\"\\nðŸ“Š Correlation Matrix:\")\n",
    "print(comparison.corr().round(3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Save Best Blended Predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save inverse RMSE weighted blend (usually best)\n",
    "blender.compute_weights(method='inverse_rmse')\n",
    "submission = blender.save_blended(\n",
    "    output_path=\"../results/ensemble_blended_inv_rmse.csv\"\n",
    ")\n",
    "\n",
    "print(f\"\\nâœ… Submission ready! Shape: {submission.shape}\")\n",
    "print(f\"   First few predictions:\")\n",
    "print(submission.head(10))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
